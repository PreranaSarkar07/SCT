# -*- coding: utf-8 -*-
"""SCT_ML_Task4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OodjzFAbwY_-bWn8SSJLbqzZ7xjNqDds
"""

!pip install kaggle

from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

# Path to your dataset zip in Google Drive
zip_path = "/content/drive/MyDrive/SCT_ML_Task4/hand_gesture_dataset.zip"

# Extraction path
extract_path = "/content/hand_gesture_dataset"
os.makedirs(extract_path, exist_ok=True)

# Unzip
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("âœ… Dataset extracted successfully at:", extract_path)

import os

# List first few classes/gesture folders
print("Folders (Gestures):", os.listdir(extract_path)[:10])

# Count images in one class
sample_class = os.listdir(extract_path)[0]
print(f"ðŸ‘‰ Example class: {sample_class}")
print("Number of images in this class:", len(os.listdir(os.path.join(extract_path, sample_class))))

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import os

data_dir = "/content/hand_gesture_dataset"

# Check total gesture classes
classes = os.listdir(data_dir)
print("Total gesture classes:", len(classes))
print("Some classes:", classes[:10])

img_size = (64, 64)   # Resize all images
batch_size = 32

datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

train_data = datagen.flow_from_directory(
    data_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode="categorical",
    subset="training"
)

val_data = datagen.flow_from_directory(
    data_dir,
    target_size=img_size,
    batch_size=batch_size,
    class_mode="categorical",
    subset="validation"
)

x, y = next(train_data)

plt.figure(figsize=(8, 8))
for i in range(9):
    plt.subplot(3, 3, i + 1)
    plt.imshow(x[i])
    plt.title(f"Class: {list(train_data.class_indices.keys())[y[i].argmax()]}")
    plt.axis("off")
plt.show()

from tensorflow.keras import layers, models

# Build CNN model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),
    layers.MaxPooling2D(2, 2),

    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),

    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D(2, 2),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(train_data.num_classes, activation='softmax')
])

# Compile model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=10,   # you can increase later for better accuracy
    verbose=1
)

plt.figure(figsize=(12,4))

# Accuracy
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title("Model Accuracy")

# Loss
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title("Model Loss")

plt.show()

val_loss, val_acc = model.evaluate(val_data)
print(f"âœ… Validation Accuracy: {val_acc*100:.2f}%")

model.save("/content/hand_gesture_model.h5")
print("âœ… Model saved as hand_gesture_model.h5")

import random
import numpy as np
from tensorflow.keras.preprocessing import image
import matplotlib.pyplot as plt
import os

# Root dataset folder
root_folder = extract_path

# Step 1: Get a random class folder
class_folder = random.choice(os.listdir(root_folder))
class_path = os.path.join(root_folder, class_folder)

# If nested, keep going deeper until we reach actual images
while os.path.isdir(os.path.join(class_path, os.listdir(class_path)[0])):
    class_path = os.path.join(class_path, os.listdir(class_path)[0])

# Step 2: Pick a random image
img_file = random.choice(os.listdir(class_path))
img_path = os.path.join(class_path, img_file)

print("âœ… Picked image:", img_path)

# Step 3: Load & preprocess
img = image.load_img(img_path, target_size=(64, 64))
img_array = image.img_to_array(img) / 255.0
img_array = np.expand_dims(img_array, axis=0)

# Step 4: Predict
pred = model.predict(img_array)
class_idx = np.argmax(pred)
class_label = list(train_data.class_indices.keys())[class_idx]

# Step 5: Show result
plt.imshow(img)
plt.title(f"Predicted: {class_label}")
plt.axis("off")
plt.show()